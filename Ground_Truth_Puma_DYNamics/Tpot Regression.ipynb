{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T17:42:51.764595Z",
     "start_time": "2019-07-10T17:42:50.152123Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\active_ml\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import lightgbm\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score\n",
    "from pyDOE import lhs\n",
    "from tpot import TPOTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T17:42:51.804201Z",
     "start_time": "2019-07-10T17:42:51.772618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:42:51] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "#Read models from string\n",
    "file_object = open('pumadyn_32nm_ML.txt', 'rb')\n",
    "str_mdl = file_object.read()\n",
    "model = pickle.loads(str_mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T17:42:52.003230Z",
     "start_time": "2019-07-10T17:42:51.809717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.355 2.355\n"
     ]
    }
   ],
   "source": [
    "#validate on the random predictions from this range\n",
    "actual_lows = {'theta1': -2.3555153, 'theta2': -2.3559486, 'theta3': -2.3556861000000002, 'theta4': -2.3554262, \n",
    "                'theta5': -2.355844, 'theta6': -2.3558267999999996, 'thetad1': -2.3556139, 'thetad2': -2.3550772999999996,\n",
    "                'thetad3': -2.3556497000000003,'thetad4': -2.3553389,'thetad5': -2.3550476000000002,'thetad6': -2.3561332999999998,\n",
    "                'tau1': -74.990634, 'tau2': -74.937538, 'tau3': -74.978278, 'tau4': -74.999533, 'tau5': -74.984873,\n",
    "                'dm1': 0.25026488,'dm2': 0.25041709, 'dm3': 0.25022193, 'dm4': 0.25007437, 'dm5': 0.25014939, 'da1': 0.25043561,\n",
    "                'da2': 0.25057928, 'da3': 0.25008744, 'da4': 0.2501664, 'da5': 0.25061589, 'db1': 0.25005398,\n",
    "                'db2': 0.25001159, 'db3': 0.25024083, 'db4': 0.25010881, 'db5': 0.2514393}\n",
    "actual_highs = {'theta1': 2.35, 'theta2': 2.355, 'theta3': 2.355, 'theta4': 2.355, \n",
    "                'theta5': 2.35,'theta6': 2.35,'thetad1': 2.35,'thetad2': 2.3540761000000003,'thetad3': 2.3547369,\n",
    "                'thetad4': 2.3557772,'thetad5': 2.3557997999999998,'thetad6': 2.3558617,'tau1': 74.985591,'tau2': 74.967958,\n",
    "                'tau3': 74.986797,'tau4': 74.99699100000001,'tau5': 74.995852,'dm1': 2.4999799,'dm2': 2.4994377999999995,\n",
    "                'dm3': 2.4999333999999998,'dm4': 2.4999981,'dm5': 2.499663,'da1': 2.4991584,'da2': 2.4996680000000002,\n",
    "                'da3': 2.4999561000000003,'da4': 2.4999662999999996,'da5': 2.4997887999999997,'db1': 2.4999776000000002,\n",
    "                'db2': 2.4996115000000003,'db3': 2.4998112999999997,'db4': 2.4999968999999997,'db5': 2.4996541000000003}\n",
    "\n",
    "variables_ = ['theta1', 'theta2', 'theta3', 'theta4', 'theta5', 'theta6', 'thetad1',\n",
    "            'thetad2', 'thetad3', 'thetad4', 'thetad5', 'thetad6', 'tau1', 'tau2',\n",
    "            'tau3', 'tau4', 'tau5', 'dm1', 'dm2', 'dm3', 'dm4', 'dm5', 'da1', 'da2',\n",
    "            'da3', 'da4', 'da5', 'db1', 'db2', 'db3', 'db4', 'db5', 'thetadd6']\n",
    "samples_ = 10000\n",
    "\n",
    "\n",
    "def random_generator(samples_, actual_lows, actual_highs, variables_, model):\n",
    "    df_doe = pd.DataFrame(columns=variables_[:-1])\n",
    "    for var in variables_[:-1]:\n",
    "        df_doe[var] = np.random.uniform(actual_lows[var], actual_highs[var],\n",
    "                                        samples_).round(3)\n",
    "    df_doe[variables_[-1]] = model.predict(df_doe).round(2)\n",
    "    return df_doe\n",
    "\n",
    "def latin_hypercube_generator(samples_, actual_lows, actual_highs, variables, model):\n",
    "    # actual_lows = {'AT': [2], 'V': [30], 'AP': [993], 'RH': [30]}  # , 'PE':[425]}\n",
    "    # actual_highs = {'AT': [35], 'V': [80], 'AP': [1033], 'RH': [100]}  # , 'PE':[495]}\n",
    "    # variables = ['AT', 'V', 'AP', 'RH', 'PE']\n",
    "    # samples_ = 100000\n",
    "    # np.random.seed(5234)\n",
    "    df_doe = pd.DataFrame(lhs(len(variables) - 1, samples=samples_, criterion='maximin'))\n",
    "    df_doe.columns = variables[:-1]\n",
    "    for col in df_doe.columns:\n",
    "        df_doe[col] = [actual_lows[col]] * df_doe.shape[0] + df_doe[col] * (actual_highs[col] - actual_lows[col])\n",
    "        df_doe[col] = df_doe[col].apply(lambda x: round(x, 3))\n",
    "    # df_doe = df_doe.reset_index()\n",
    "    df_doe.columns = variables[:-1]\n",
    "    df_doe[variables[-1]] = model.predict(df_doe).round(3)\n",
    "    return df_doe\n",
    "\n",
    "def normalize(input_array):\n",
    "    mean = np.mean(input_array, axis=0)\n",
    "    std = np.std(input_array, axis=0)\n",
    "\n",
    "    # scikit-learn measure to handle zeros in scale: def _handle_zeros_in_scale(scale, copy=True)\n",
    "    # https://github.com/scikit-learn/scikit-learn/blob/7389dbac82d362f296dc2746f10e43ffa1615660/sklearn/preprocessing/data.py#L70\n",
    "    if np.isscalar(std):\n",
    "        if std == .0:\n",
    "            std = 1.\n",
    "    elif isinstance(std, np.ndarray):\n",
    "        std = std.copy()\n",
    "        std[std == 0.0] = 1.0\n",
    "\n",
    "    data_norm = (input_array - mean) / std\n",
    "    return mean, std, data_norm\n",
    "\n",
    "df = random_generator(samples_, actual_lows, actual_highs, variables_, model)\n",
    "print(df['thetad3'].min(), df['thetad3'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T17:43:20.096807Z",
     "start_time": "2019-07-10T17:42:52.005735Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_test_set = 1000\n",
    "size_train_set = 300\n",
    "reference_regr = model\n",
    "np.random.seed(12345)\n",
    "test_data = latin_hypercube_generator(size_test_set, actual_lows, actual_highs, variables_, reference_regr)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "np.random.seed(2)\n",
    "train_data = np.array(random_generator(size_train_set, actual_lows, actual_highs, variables_, reference_regr))\n",
    "mean_data, std_data, data_all_norm = normalize(np.concatenate((train_data, test_data), axis=0))\n",
    "\n",
    "X_train = train_data[:, :-1]\n",
    "X_test = test_data[:, :-1]\n",
    "y_train = train_data[:, -1].reshape((-1, 1))\n",
    "y_test = test_data[:, -1].reshape((-1, 1))\n",
    "\n",
    "# NORMALIZED DATA\n",
    "X_train_norm = data_all_norm[:train_data.shape[0], :-1]\n",
    "y_train_norm = data_all_norm[:train_data.shape[0], -1].reshape((-1, 1))\n",
    "X_test_norm = data_all_norm[train_data.shape[0]:, :-1]\n",
    "y_test_norm = data_all_norm[train_data.shape[0]:, -1].reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-10T17:51:16.856907Z",
     "start_time": "2019-07-10T17:51:16.843371Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regressor_config_dict = {\n",
    "\n",
    "    'sklearn.linear_model.ElasticNetCV': {\n",
    "        'l1_ratio': np.arange(0.0, 1.01, 0.05),\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    },\n",
    "\n",
    "    'sklearn.neighbors.KNeighborsRegressor': {\n",
    "        'n_neighbors': range(1, 101),\n",
    "        'weights': [\"uniform\", \"distance\"],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "\n",
    "    'sklearn.linear_model.LassoLarsCV': {\n",
    "        'normalize': [True, False]\n",
    "    },\n",
    "\n",
    "    'sklearn.svm.LinearSVR': {\n",
    "        'loss': [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "        'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1.]\n",
    "    },\n",
    "\n",
    "\n",
    "    'sklearn.linear_model.RidgeCV': {\n",
    "    },\n",
    "\n",
    "\n",
    "    # Preprocesssors\n",
    "    'sklearn.preprocessing.Binarizer': {\n",
    "        'threshold': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.FastICA': {\n",
    "        'tol': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.cluster.FeatureAgglomeration': {\n",
    "        'linkage': ['ward', 'complete', 'average'],\n",
    "        'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MaxAbsScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.MinMaxScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.Normalizer': {\n",
    "        'norm': ['l1', 'l2', 'max']\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.Nystroem': {\n",
    "        'kernel': ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid'],\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05),\n",
    "        'n_components': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.decomposition.PCA': {\n",
    "        'svd_solver': ['randomized'],\n",
    "        'iterated_power': range(1, 11)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.PolynomialFeatures': {\n",
    "        'degree': [2],\n",
    "        'include_bias': [False],\n",
    "        'interaction_only': [False]\n",
    "    },\n",
    "\n",
    "    'sklearn.kernel_approximation.RBFSampler': {\n",
    "        'gamma': np.arange(0.0, 1.01, 0.05)\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.RobustScaler': {\n",
    "    },\n",
    "\n",
    "    'sklearn.preprocessing.StandardScaler': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.ZeroCount': {\n",
    "    },\n",
    "\n",
    "    'tpot.builtins.OneHotEncoder': {\n",
    "        'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        'sparse': [False],\n",
    "        'threshold': [10]\n",
    "    },\n",
    "\n",
    "\n",
    "    # Selectors\n",
    "    'sklearn.feature_selection.SelectFwe': {\n",
    "        'alpha': np.arange(0, 0.05, 0.001),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_regression': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.SelectPercentile': {\n",
    "        'percentile': range(1, 100),\n",
    "        'score_func': {\n",
    "            'sklearn.feature_selection.f_regression': None\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'sklearn.feature_selection.VarianceThreshold': {\n",
    "        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    },\n",
    "\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T07:28:05.988220Z",
     "start_time": "2019-07-11T07:21:28.009816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressor_dx = TPOTRegressor(generations=30, population_size=30, \n",
    "                             config_dict=regressor_config_dict, \n",
    "                             cv=5, random_state=42,\n",
    "                             verbosity=2, scoring='r2',\n",
    "                            n_jobs = -1) # max_time_mins=5\n",
    "\n",
    "regressor_dx.fit(X_train_norm, y_train_norm)\n",
    "\n",
    "print(regressor_dx.score(X_test_norm, y_test_norm))\n",
    "\n",
    "regressor_dx.export('tpot_reg_dx_result_long_run.py')\n",
    "# see which pipelines were evaluated:\n",
    "print(regressor_dx.evaluated_individuals_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_test_set = 1000\n",
    "reference_regr = model\n",
    "np.random.seed(12345)\n",
    "test_data = latin_hypercube_generator(size_test_set, actual_lows, actual_highs, variables_, reference_regr)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-11T09:18:00.526008Z",
     "start_time": "2019-07-11T09:17:58.950316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17850507050197362\n"
     ]
    }
   ],
   "source": [
    "size_train_set = 400\n",
    "\n",
    "np.random.seed(1)\n",
    "train_data = np.array(random_generator(size_train_set, actual_lows, actual_highs, variables_, reference_regr))\n",
    "mean_data, std_data, data_all_norm = normalize(np.concatenate((train_data, test_data), axis=0))\n",
    "\n",
    "X_train = train_data[:, :-1]\n",
    "X_test = test_data[:, :-1]\n",
    "y_train = train_data[:, -1].reshape((-1, 1))\n",
    "y_test = test_data[:, -1].reshape((-1, 1))\n",
    "\n",
    "# NORMALIZED DATA\n",
    "X_train_norm = data_all_norm[:train_data.shape[0], :-1]\n",
    "y_train_norm = data_all_norm[:train_data.shape[0], -1].reshape((-1, 1))\n",
    "X_test_norm = data_all_norm[train_data.shape[0]:, :-1]\n",
    "y_test_norm = data_all_norm[train_data.shape[0]:, -1].reshape((-1, 1))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, RBF\n",
    "\n",
    "# NOTE: Make sure that the class is labeled 'target' in the data file\n",
    "#tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "#features = tpot_data.drop('target', axis=1).values\n",
    "#training_features, testing_features, training_target, testing_target = \\\n",
    "#            train_test_split(features, tpot_data['target'].values, random_state=42)\n",
    "\n",
    "# Average CV score on the training set was:0.8623827025013939\n",
    "model = make_pipeline(\n",
    "    SelectFromModel(estimator=ExtraTreesRegressor(max_features=1.0, n_estimators=100), threshold=0.1),\n",
    "    KNeighborsRegressor(n_neighbors=10, p=2, weights=\"uniform\")\n",
    ")\n",
    "kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\n",
    "model = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "model = GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n",
    "    kernel=1**2 + Matern(length_scale=2, nu=1.5) + WhiteKernel(noise_level=1),\n",
    "    n_restarts_optimizer=0, normalize_y=False,\n",
    "    optimizer='fmin_l_bfgs_b', random_state=None)\n",
    "\n",
    "model.fit(X_train_norm, y_train_norm)\n",
    "print(model.score(X_test_norm, y_test_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:active_ml]",
   "language": "python",
   "name": "conda-env-active_ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
